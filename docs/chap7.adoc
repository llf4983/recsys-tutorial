== 神经网络BP反向传播算法原理和详细推导流程

=== 反向传播算法和BP网络简介

误差反向传播算法简称反向传播算法（即BP算法）。使用反向传播算法的多层感知器又称为BP神经网络。BP算法是一个迭代算法，它的基本思想为：

. 先计算每一层的状态和激活值，直到最后一层（即信号是前向传播的）；
. 计算每一层的误差，误差的计算过程是从最后一层向前推进的（这就是反向传播算法名字的由来）；
. 更新参数（目标是误差变小）。迭代前面两个步骤，直到满足停止准则（比如相邻两次迭代的误差的差别很小）。

本文的记号说明：

* stem:[n_l]表示第stem:[l]层神经元的个数；
* stem:[f(\cdot)]表示神经元的激活函数；
* stem:[W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}]表示第stem:[l-1]层到第stem:[l]层的权重矩阵；
* stem:[w_{ij}^{(l)}]是权重矩阵stem:[W^{(l)}]中的元素，表示第stem:[l-1]层第stem:[j]个神经元到第stem:[l]层第stem:[i]个神经元的连接的权重（注意标号的顺序）；
* stem:[\textbf{b}^{(l)}=(b_1^{(l)},b_2^{(l)},\dots,b_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}]表示stem:[l-1]层到第stem:[l]层的偏置；
* stem:[\textbf{z}^{(l)}=(z_1^{(l)},z_2^{(l)},\dots,z_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}]表示stem:[l]层神经元的状态；
* stem:[\textbf{a}^{(l)}=(a_1^{(l)},a_2^{(l)},\dots,a_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}]表示stem:[l]层神经元的激活值（即输出值）；

**关于记号的特别注意**：不同的文献所采用的记号可能不同，这将导致不同文献的公式结论可能不同。如Andrew Ng的教程中的stem:[W^{(l)}]表示的是第stem:[l]层到第stem:[l+1]层的权重矩阵。又如，本文用“下标”来标记一个向量的不同分量，而有一些资料却用“上标”来标记向量的不同分量。

下面以三层感知器(即只含有一个隐藏层的多层感知器)为例介绍“反向传播算法(BP算法)”。

三层感知机如图1所示。例子中，输入数据stem:[\textbf{x}=(x_1,x_2,x_3)^T]是3维的（对于第一层，可以认为stem:[a_i^{(1)}=x_i]），唯一的隐藏层有3个节点，输出数据是2维的。

image::dnn8.png[]

=== 信息前向传播

显然，图1所示神经网络的第2层神经元的状态及激活值可以通过下面的计算得到：

[stem]
++++
z_1^{(2)}=w_{11}^{(2)}x_1+w_{12}^{(2)}x_2+w_{13}^{(2)}x_3+b_1^{(2)} \\
z_2^{(2)}=w_{21}^{(2)}x_1+w_{22}^{(2)}x_2+w_{23}^{(2)}x_3+b_2^{(2)} \\
z_3^{(2)}=w_{31}^{(2)}x_1+w_{32}^{(2)}x_2+w_{33}^{(2)}x_3+b_3^{(2)} \\
a_1^{(2)}=f(z_1^{(2)}) \\
a_2^{(2)}=f(z_2^{(2)}) \\
a_3^{(2)}=f(z_3^{(2)})
++++

类似的，第3层神经元的状态及激活值可以通过下面的计算得到：

[stem]
++++
z_1^{(3)}=w_{11}^{(3)}a_1^{(2)}+w_{12}^{(2)}a_2^{(2)}+w_{13}^{(2)}a_3^{(2)}+b_1^{(3)} \\
z_2^{(3)}=w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(2)}a_3^{(2)}+b_2^{(3)} \\
a_1^{(3)}=f(z_1^{(3)}) \\
a_2^{(3)}=f(z_2^{(3)})
++++

可以总结出，第stem:[l(2 \le l \le L)]层神经元的状态及激活值为（下面式子是向量表示形式）：

[stem]
++++
\textbf{z}^{(l)}=W^{(l)} \textbf{a}^{(l-1)}+\textbf{b}^{(l)} \\
\textbf{a}^{(l)}=f(\textbf{z}^{(l)})
++++

对于stem:[L]层感知机，网络的最终输出为stem:[\textbf{a}^{(L)}]。前馈神经网络中信息的前向传递过程如下：

[stem]
++++
\textbf{x}=\textbf{a}^{(1)} \rightarrow \textbf{z}^{(2)} \rightarrow \dots \rightarrow \textbf{a}^{(L-1)} \rightarrow \textbf{z}^{(L)} \rightarrow \textbf{a}^{(L)}=y
++++

=== 误差反向传播

“信息向前传播”讲的是已知各个神经元的参数后，如何得到神经网络的输出。但怎么得到各个神经元的参数呢？“误差反向传播”算法解决的就是这个问题。

假设训练数据为stem:[{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(i)},y^{(i)}),\dots,(x^{(N)},y^{(N)})}]，即共有stem:[N]个。又假设输出数据为stem:[n_L]维的，即stem:[\textbf{y}^{(i)}=(y_1^{(i)},\dots,y_{n_L}^{(i)})^T]。

对某一个训练数据stem:[(\textbf{x}^{(i)},\textbf{y}^{(i)})]来说，其代价函数可写为：

[stem]
++++
\begin{align}
E_{(i)} &= \frac{1}{2} \parallel \textbf{y}^{(i)} - \textbf{(o)}^{(i)} \parallel \\
&= \frac{1}{2} \sum_{k=1}^{n_L}(y_k^{(i)} - o_k^{(i)})^2
\end{align}
++++

说明1：stem:[\textbf{y}^{(i)}]为期望的输出（是训练数据给出的已知值），stem:[\textbf{o}^{(i)}]为神经网络对输入stem:[\textbf{x}^{(i)}]产生的实际输出。

说明2：代价函数中的系数stem:[\frac{1}{2}]显然是不必要的，它的存在仅仅是为了后续计算时更方便。

说明3：以图1所示的神经网络为例子，stem:[n_L=2,\textbf{y}^{(i)}=(y_1^{(i)},y_2^{(i)})^T]，从而有stem:[E_{(i)}=\frac{1}{2}(y_1^{(i)}-a_1^{(3)})^2+\frac{1}{2}(y_2^{(i)}-a_2^{(3)})^2]，如果展开到隐藏层，则有stem:[E_{(i)}=\frac{1}{2}(y_1^{(i)}-f(w_{11}^{(3)}a_1^{(2)}+w_{12}^{(3)}a_2^{(2)}+w_{13}^{(3)}a_3^{(2)}+b_1^{(3)}))^2 + \frac{1}{2}(y_2^{(i)}-f(w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(3)}a_3^{(2)}+b_2^{(3)}))^2]，还可以进一步展开到输入层（替换掉stem:[a_1^{(2)},a_2^{(2)},a_3^{(2)}]即可），最后可得：**代价函数stem:[E_{(i)}]仅和权重矩阵stem:[W^{(l)}]和偏置向量stem:[b^{(l)}]相关，调整权重和偏置可以减少或增大代价（误差）。**

显然，所有训练数据的总体（平均）代价可写为：

[stem]
++++
E_{total}=\frac{1}{N}\sum_{i=1}^N E_{(i)}
++++

我们的目标就是调整权重和偏置使总体代价（误差）变小，求得总体代价取最小值时对应的各个神经元的参数（即权重和偏置）。

如果采用梯度下降法（在这里，又可称为“批量梯度下降法”），可以用下面公式更新参数stem:[w_{ij}^{(l)},b_i^{(l)},2 \le l \le L]：

[stem]
++++
\begin{align}
W^{(l)} &= W^{(l)} - \mu\frac{\partial E_{total}}{\partial W^{(l)}} \\
&= W^{(l)} - \frac{\mu}{N}\sum_{i=1}^N \frac{\partial E_{(i)}}{\partial W^{(l)}} \\
b^{(l)} &= b^{(l)} - \mu\frac{\partial E_{total}}{\partial b^{(l)}} \\
&= b^{(l)} - \frac{\mu}{N}\sum_{i=1}^N\frac{\partial E_{(i)}}{\partial b^{(l)}}
\end{align}
++++

由上面公式可知，只需求得每一个训练数据的代价函数stem:[E_{(i)}]对参数的偏导数stem:[\frac{\partial E_{(i)}}{\partial W^{(l)}},\frac{\partial E_{(i)}}{\partial b^{(l)}}]即可得到参数的迭代更新公式。

**为简单起见，在下文的推导中，我们去掉stem:[E_{(i)}]的下标，直接记为E（要理解它是单个训练数据的误差）。**

下面将介绍用“反向传播算法”求解单个训练数据误差对参数的偏导数stem:[\frac{\partial E}{\partial W^{(l)}}]和stem:[\frac{\partial E}{\partial b^{(l)}}]的过程。我们求解一个简单的情况：图1所示神经网络，最后再归纳出通用公式。

==== 输出层的权重参数更新

把stem:[E]展开到隐藏层，有：

[stem]
++++
\begin{align}
E &= \frac{1}{2} \parallel \textbf{y}-\textbf{o} \parallel \\
&= \frac{1}{2} \parallel \textbf{y}-\textbf{a}^{(3)} \parallel \\
&= \frac{1}{2}(y_1^{(i)}-a_1^{(3)})^2+\frac{1}{2}(y_2^{(i)}-a_2^{(3)})^2 \\
&= \frac{1}{2}((y_1-f(z_1^{(3)}))^2+(y_2-f(z_2^{(3)}))^2) \\
&= \frac{1}{2}(y_1^{(i)}-f(w_{11}^{(3)}a_1^{(2)}+w_{12}^{(3)}a_2^{(2)}+w_{13}^{(3)}a_3^{(2)}+b_1^{(3)}))^2+\frac{1}{2}(y_2^{(i)}-f(w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(3)}a_3^{(2)}+b_2^{(3)}))^2
\end{align}
++++

由求导的链式法则，对“输出层神经元的权重参数”求偏导，有：

[stem]
++++
\begin{align}
\frac{\partial E}{\partial w_{11}^{(3)}} &= \frac{1}{2} \cdot 2(y_1-a_1^{(3)})(-\frac{\partial a_1^{(3)}}{\partial w_{11}^{(3)}}) \\
&= -(y_1-a_1^{(3)})f'(z_1^{3})\frac{\partial z_1^{(3)}}{\partial w_{11}^{(3)}} \\
&= -(y_1-a_1^{(3)})f'(z_1^{3})a_1^{(2)}
\end{align}
++++